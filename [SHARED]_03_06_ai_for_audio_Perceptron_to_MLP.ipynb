{"cells":[{"cell_type":"markdown","id":"e4be4e51","metadata":{"id":"e4be4e51"},"source":["# Super Simple Perceptron ðŸ”¥\n","\n","Let's start with the perceptron and see what happens to the input as it goes through the model:\n","\n","1. **Input**: The perceptron gets input values, as a vector $ \\mathbf{x} $.\n","\n","   Here's the example vector with two inputs: $ \\mathbf{x} = [0.5, 1.5] $.\n","\n","2. **Weights and Bias**:\n","   - **Weights ($ \\mathbf{w} $)**: The perceptron learns these parameters during training which sets the importance, or how much `weight`, we give each input.\n","   - **Bias ($ b $)**: This is the other important parameter that helps the model to fit the data better by shifting the activation function.\n","   > Essentially the bias adjusts the output regardless of the inputs by shifting the activation function, especially when the input values are zero or do not fully explain the output on their own. ðŸ¤”\n","\n","\n","   For our example, let's use weights $ \\mathbf{w} = [0.8, 0.2] $ and bias $ b = 0.5 $.\n","\n","3. **Weighted Sum**: The perceptron calculates a weighted sum of the inputs and adds the bias. The formula is:\n","\n","   $$\n","   z = \\mathbf{w} \\cdot \\mathbf{x} + b\n","   $$\n","\n","   Substituting the values:\n","\n","   $$\n","   z = (0.8 \\times 0.5) + (0.2 \\times 1.5) + 0.5 = 0.4 + 0.3 + 0.5 = 1.2\n","   $$\n","\n","4. **Activation Function**: The weighted sum is what we pass through the activation function. Common choices, whicch we'll cover later, include the step function, sigmoid, or ReLU. For simplicity, we'll use a step function:\n","\n","   $$\n","   \\text{output} = \\begin{cases}\n","      1 & \\text{if } z \\geq 0 \\\\\n","      0 & \\text{if } z < 0\n","   \\end{cases}\n","   $$\n","\n","   Since $ z = 1.2 $, which is greater than 0, the output is 1.\n","   Because,anything above 0 is 1 ðŸ’ª\n"]},{"cell_type":"code","execution_count":null,"id":"7d432585","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7d432585","executionInfo":{"status":"ok","timestamp":1723128516171,"user_tz":300,"elapsed":5,"user":{"displayName":"Jonathan Barrios","userId":"10019067611471099465"}},"outputId":"f764af64-8a9a-4880-9765-91f8a5d959f7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Weighted sum: 1.2000000000000002\n","Output: 1\n"]}],"source":["\n","import numpy as np\n","\n","# Define input, weights, and bias\n","x = np.array([0.5, 1.5])\n","w = np.array([0.8, 0.2])\n","b = 0.5\n","\n","# Calculate weighted sum\n","z = np.dot(w, x) + b\n","print(f'Weighted sum: {z}')\n","\n","# Step activation function\n","output = 1 if z >= 0 else 0\n","print(f'Output: {output}')\n"]},{"cell_type":"code","source":["x[0], w[0], b, z"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nxkke1qrdrxy","executionInfo":{"status":"ok","timestamp":1723128555574,"user_tz":300,"elapsed":311,"user":{"displayName":"Jonathan Barrios","userId":"10019067611471099465"}},"outputId":"39ff7990-4dd3-48f0-e390-be9142355f60"},"id":"nxkke1qrdrxy","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.5, 0.8, 0.5, 1.2000000000000002)"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","id":"4a82ebf8","metadata":{"id":"4a82ebf8"},"source":["\n","# **Extending to Multilayer Perceptron (MLP)**\n","\n","Here, we'll investigate out how `inputs` are transformed from a perceptron to a Multilayer Perceptron (MLP) and whats happening inside!\n","\n","1. **Input Layer**: Similar to a single perceptron, the MLP starts with an input layerâ€”the same input $ \\mathbf{x} = [0.5, 1.5] $.\n","\n","2. **Hidden Layer(s)**: MLPs have one or more hidden layers. Each neuron in a hidden layer performs the same operations as a single perceptron.\n","\n","   Let's take a look at an MLP with one hidden layer containing two neurons.\n","\n","   **Neuron 1 in Hidden Layer:**\n","   - Weights: $ \\mathbf{w_1} = [0.8, 0.2] $\n","   - Bias: $ b_1 = 0.5 $\n","\n","   **Neuron 2 in Hidden Layer:**\n","   - Weights: $ \\mathbf{w_2} = [0.4, 0.9] $\n","   - Bias: $ b_2 = -0.3 $\n","\n","   **Calculations for Hidden Layer:**\n","   - Neuron 1: $ z_1 = (0.8 \\times 0.5) + (0.2 \\times 1.5) + 0.5 = 1.2 $\n","   - Neuron 2: $ z_2 = (0.4 \\times 0.5) + (0.9 \\times 1.5) - 0.3 = 1.4 $\n","\n","   > Applying a ReLU activation function turns negative numbers to 0. ReLU applied to `[-1  0  1  2]` => `[0 0 1 2]`.\n","   \n","   > ReLU is often used in hidden layers of neural networks because it helps to mitigate the vanishing gradient problem and introduces non-linearity into the model.\n","\n","Here we apply ReLU:\n","   $$\n","   \\text{output}_1 = \\max(0, 1.2) = 1.2\n","   $$\n","   $$\n","   \\text{output}_2 = \\max(0, 1.4) = 1.4\n","   $$\n","\n","3. **Output Layer**: The outputs from the hidden layer are then fed into the output layer.\n","\n","   **Neuron in Output Layer:**\n","   - Weights: $ \\mathbf{w_3} = [0.3, 0.7] $\n","   - Bias: $ b_3 = 0.1 $\n","\n","   **Calculations for Output Layer:**\n","   - Input to this layer: $ [1.2, 1.4] $\n","\n","   $$\n","   z_3 = (0.3 \\times 1.2) + (0.7 \\times 1.4) + 0.1 = 1.34\n","   $$\n","\n","   Applying a sigmoid activation function for binary classification:\n","\n","   $$\n","   \\text{output} = \\frac{1}{1 + e^{-1.34}} \\approx 0.79\n","   $$\n"]},{"cell_type":"code","execution_count":1,"id":"707b8f00","metadata":{"id":"707b8f00","executionInfo":{"status":"ok","timestamp":1723153892128,"user_tz":300,"elapsed":7,"user":{"displayName":"Jonathan Barrios","userId":"10019067611471099465"}},"outputId":"52140734-d9f5-47d8-f01b-836848b4883b","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Output: 0.7916664907298545\n"]}],"source":["import numpy as np\n","\n","# Define input\n","x = np.array([0.5, 1.5])\n","\n","# Weights and biases for hidden layer\n","w1 = np.array([0.8, 0.2])\n","b1 = 0.5\n","w2 = np.array([0.4, 0.9])\n","b2 = -0.3\n","\n","# Calculate outputs for hidden layer\n","z1 = np.dot(w1, x) + b1\n","z2 = np.dot(w2, x) + b2\n","\n","# Apply ReLU activation\n","output1 = max(0, z1)\n","output2 = max(0, z2)\n","\n","# Weights and bias for output layer\n","w3 = np.array([0.3, 0.7])\n","b3 = 0.1\n","\n","# Calculate output for output layer\n","z3 = np.dot(w3, np.array([output1, output2])) + b3\n","\n","# Apply sigmoid activation\n","output = 1 / (1 + np.exp(-z3))\n","\n","print(f'Output: {output}')\n"]},{"cell_type":"markdown","id":"2769e21b","metadata":{"id":"2769e21b"},"source":["\n","### Explanation Recap\n","\n","- **Perceptron:** Single layer of computation, one neuron, that processes inputs through weighted sums and activation functions.\n","- **MLP:** Multiple layers, many neurons, where each layerâ€™s output becomes the next layerâ€™s input, allowing the network to learn complex patterns.\n","\n","I hope this helps you understand how inputs are transformed as they travel through the network. ðŸ”¥\n"]}],"metadata":{"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}